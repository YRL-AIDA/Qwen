# -*- coding: utf-8 -*-
"""DocOwl.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1B2Oqgp3u9rYBC2zg2S7_Vfpu4UVzs9F7
"""

from transformers import AutoModelForCausalLM, AutoTokenizer
from transformers.generation import GenerationConfig
import torch
torch.manual_seed(1234)

# Note: The default behavior now has injection attack prevention off.
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen-VL-Chat", trust_remote_code=True)

# use bf16
# model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-VL-Chat", device_map="auto", trust_remote_code=True, bf16=True).eval()
# use fp16
# model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-VL-Chat", device_map="auto", trust_remote_code=True, fp16=True).eval()
# use cpu only
# model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-VL-Chat", device_map="cpu", trust_remote_code=True).eval()
# use cuda device
model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen-VL-Chat-Int4",
    device_map="auto",
    trust_remote_code=True
).eval()

# Specify hyperparameters for generation
model.generation_config = GenerationConfig.from_pretrained("Qwen/Qwen-VL-Chat", trust_remote_code=True)

# 1st dialogue turn
query = tokenizer.from_list_format([
    {'image': './image16.jpg'}, # Either a local path or an url
    {'text': 'Что это?'},
])
response, history = model.chat(tokenizer, query=query, history=None)
print(response)
# 图中是一名女子在沙滩上和狗玩耍，旁边是一只拉布拉多犬，它们处于沙滩上。

# 2nd dialogue turn
response, history = model.chat(tokenizer, 'Кто поставщик?', history=history)
print(response)
# <ref>击掌</ref><box>(536,509),(588,602)</box>
image = tokenizer.draw_bbox_on_latest_picture(response, history)
if image:
  image.save('1.jpg')
else:
  print("no box")

!pip install optimum

!pip uninstall -y sentence-transformers transformers accelerate


!pip install transformers==4.41.0 accelerate==0.29.3 peft==0.7.0


!pip install auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/

!ls

import os
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "max_split_size_mb:128,expandable_segments:True"
os.environ["WANDB_DISABLED"] = "true"
import torch
import gc

torch.cuda.empty_cache()
gc.collect()
!python finetune.py \
--model_name_or_path "Qwen/Qwen-VL-Chat-Int4" \
--data_path "./Dataset.json" \
--output_dir "./suppliers_lora" \
--use_lora \
--lora_r 8 \
--lora_alpha 4 \
--per_device_train_batch_size 1 \
--gradient_accumulation_steps 1 \
--num_train_epochs 20

!pip install -r requirements.txt

!git clone https://github.com/QwenLM/Qwen-VL.git

# Commented out IPython magic to ensure Python compatibility.
# %cd Qwen-VL

!pip install peft

!pip install deepspeed

# Commented out IPython magic to ensure Python compatibility.
# %cd ..

